{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lyjpr98C7mVE",
        "outputId": "44afceaf-e0ab-4b29-fe0e-a92244bee399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "3SLLLpm18baz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "lmtgINH08pbv",
        "outputId": "d23e8037-9648-4580-d280-86a34f1bc3ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7bd2d1d6b450>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ea2a593e95c7:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1) BUILDING CLASSIFICATION MODEL USING SPARK"
      ],
      "metadata": {
        "id": "9KZiaQgx9tDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['species'] = iris.target\n"
      ],
      "metadata": {
        "id": "_yDohxar9kLI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df = spark.createDataFrame(df)\n",
        "spark_df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh_y4zfR-Tst",
        "outputId": "65524454-6221-4231-a592-b0425c412889"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+----------------+-----------------+----------------+-------+\n",
            "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|species|\n",
            "+-----------------+----------------+-----------------+----------------+-------+\n",
            "|              5.1|             3.5|              1.4|             0.2|      0|\n",
            "|              4.9|             3.0|              1.4|             0.2|      0|\n",
            "|              4.7|             3.2|              1.3|             0.2|      0|\n",
            "|              4.6|             3.1|              1.5|             0.2|      0|\n",
            "|              5.0|             3.6|              1.4|             0.2|      0|\n",
            "+-----------------+----------------+-----------------+----------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"species\", outputCol=\"label\")\n",
        "indexed_df = indexer.fit(spark_df).transform(spark_df)\n",
        "indexed_df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSyHL6OC-V7N",
        "outputId": "1734308b-1bd1-456a-f1e7-e667a9e4f031"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+----------------+-----------------+----------------+-------+-----+\n",
            "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|species|label|\n",
            "+-----------------+----------------+-----------------+----------------+-------+-----+\n",
            "|              5.1|             3.5|              1.4|             0.2|      0|  0.0|\n",
            "|              4.9|             3.0|              1.4|             0.2|      0|  0.0|\n",
            "|              4.7|             3.2|              1.3|             0.2|      0|  0.0|\n",
            "|              4.6|             3.1|              1.5|             0.2|      0|  0.0|\n",
            "|              5.0|             3.6|              1.4|             0.2|      0|  0.0|\n",
            "+-----------------+----------------+-----------------+----------------+-------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=iris.feature_names,\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "assembled_df = assembler.transform(indexed_df)\n",
        "assembled_df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8zbpp_n-bg1",
        "outputId": "dfc046d2-5edb-47db-b34b-b33db3442aa2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+----------------+-----------------+----------------+-------+-----+-----------------+\n",
            "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|species|label|         features|\n",
            "+-----------------+----------------+-----------------+----------------+-------+-----+-----------------+\n",
            "|              5.1|             3.5|              1.4|             0.2|      0|  0.0|[5.1,3.5,1.4,0.2]|\n",
            "|              4.9|             3.0|              1.4|             0.2|      0|  0.0|[4.9,3.0,1.4,0.2]|\n",
            "|              4.7|             3.2|              1.3|             0.2|      0|  0.0|[4.7,3.2,1.3,0.2]|\n",
            "|              4.6|             3.1|              1.5|             0.2|      0|  0.0|[4.6,3.1,1.5,0.2]|\n",
            "|              5.0|             3.6|              1.4|             0.2|      0|  0.0|[5.0,3.6,1.4,0.2]|\n",
            "+-----------------+----------------+-----------------+----------------+-------+-----+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = assembled_df.randomSplit([0.8, 0.2], seed=42)\n"
      ],
      "metadata": {
        "id": "Jtd1nA_s-efm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "# Initialize Decision Tree Classifier with regularization parameters\n",
        "dt = DecisionTreeClassifier(\n",
        "    labelCol=\"label\",\n",
        "    featuresCol=\"features\",\n",
        "    maxDepth=5,  # Limit tree depth\n",
        "    minInstancesPerNode=10,  # Minimum instances per node\n",
        "    minInfoGain=0.01,  # Minimum information gain for a split\n",
        "    maxBins=32  # Maximum number of bins\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model = dt.fit(train_data)\n",
        "\n",
        "# Make predictions on training data\n",
        "train_predictions = model.transform(train_data)\n",
        "\n",
        "# Evaluate training accuracy\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
        ")\n",
        "train_accuracy = evaluator.evaluate(train_predictions)\n",
        "print(f\"Training Accuracy = {train_accuracy:.2f}\")\n",
        "\n",
        "# Make predictions on test data\n",
        "test_predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate test accuracy\n",
        "test_accuracy = evaluator.evaluate(test_predictions)\n",
        "print(f\"Test Accuracy = {test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhpRXvZ9Ad8R",
        "outputId": "ee51ba47-5107-431e-f01e-2e09504ba473"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy = 0.95\n",
            "Test Accuracy = 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Extract feature importances\n",
        "importances = model.featureImportances\n",
        "importance_values = importances.toArray()\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "importance_df = pd.DataFrame({\n",
        "    \"Feature\": iris.feature_names,\n",
        "    \"Importance\": importance_values\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qL2pGDRAnVh",
        "outputId": "7a10654b-75f7-40d5-8c76-d721473bc310"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature Importance:\n",
            "             Feature  Importance\n",
            "2  petal length (cm)         1.0\n",
            "0  sepal length (cm)         0.0\n",
            "1   sepal width (cm)         0.0\n",
            "3   petal width (cm)         0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Define a parameter grid\n",
        "paramGrid = (ParamGridBuilder()\n",
        "    .addGrid(dt.maxDepth, [5, 10])\n",
        "    .addGrid(dt.minInstancesPerNode, [5, 10])\n",
        "    .addGrid(dt.minInfoGain, [0.0, 0.01])\n",
        "    .addGrid(dt.maxBins, [32, 64])\n",
        "    .build())\n",
        "\n",
        "# Initialize CrossValidator\n",
        "cv = CrossValidator(\n",
        "    estimator=dt,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=3  # Use 3-fold cross-validation\n",
        ")\n",
        "\n",
        "# Train the model with cross-validation\n",
        "cv_model = cv.fit(train_data)\n",
        "\n",
        "# Get the best model\n",
        "best_model = cv_model.bestModel\n",
        "\n",
        "# Evaluate on test data\n",
        "test_predictions = best_model.transform(test_data)\n",
        "test_accuracy = evaluator.evaluate(test_predictions)\n",
        "print(f\"Test Accuracy with Best Model = {test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOLWbrdXAp8s",
        "outputId": "280b1c06-2440-428b-f5d6-ef2dcf246412"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy with Best Model = 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2)Build a Clustering Model With Spark"
      ],
      "metadata": {
        "id": "-4iqfdHjBcCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "spark_df = spark.createDataFrame(df)\n",
        "spark_df.show(5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXVgSknJCEAH",
        "outputId": "811a4408-f37b-4e95-cf29-a61a4024f2a6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+----------------+-----------------+----------------+\n",
            "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|\n",
            "+-----------------+----------------+-----------------+----------------+\n",
            "|              5.1|             3.5|              1.4|             0.2|\n",
            "|              4.9|             3.0|              1.4|             0.2|\n",
            "|              4.7|             3.2|              1.3|             0.2|\n",
            "|              4.6|             3.1|              1.5|             0.2|\n",
            "|              5.0|             3.6|              1.4|             0.2|\n",
            "+-----------------+----------------+-----------------+----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Convert features to a single vector column\n",
        "assembler = VectorAssembler(inputCols=iris.feature_names, outputCol=\"features\")\n",
        "assembled_df = assembler.transform(spark_df)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "scaler_model = scaler.fit(assembled_df)\n",
        "scaled_df = scaler_model.transform(assembled_df)\n",
        "scaled_df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqfU0BY6CSDZ",
        "outputId": "89965822-c19f-4cc0-e211-eb3a7e8a8bfb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+----------------+-----------------+----------------+-----------------+--------------------+\n",
            "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|         features|     scaled_features|\n",
            "+-----------------+----------------+-----------------+----------------+-----------------+--------------------+\n",
            "|              5.1|             3.5|              1.4|             0.2|[5.1,3.5,1.4,0.2]|[6.15892840883878...|\n",
            "|              4.9|             3.0|              1.4|             0.2|[4.9,3.0,1.4,0.2]|[5.9174018045706,...|\n",
            "|              4.7|             3.2|              1.3|             0.2|[4.7,3.2,1.3,0.2]|[5.67587520030241...|\n",
            "|              4.6|             3.1|              1.5|             0.2|[4.6,3.1,1.5,0.2]|[5.55511189816831...|\n",
            "|              5.0|             3.6|              1.4|             0.2|[5.0,3.6,1.4,0.2]|[6.03816510670469...|\n",
            "+-----------------+----------------+-----------------+----------------+-----------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "kmeans = KMeans(k=3, featuresCol=\"scaled_features\", predictionCol=\"prediction\")\n",
        "model = kmeans.fit(scaled_df)\n",
        "predictions = model.transform(scaled_df)\n",
        "predictions.select(\"scaled_features\", \"prediction\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tACnvvMUCWqC",
        "outputId": "8fad11c9-9504-41d6-8bae-89c8188d018f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+\n",
            "|     scaled_features|prediction|\n",
            "+--------------------+----------+\n",
            "|[6.15892840883878...|         1|\n",
            "|[5.9174018045706,...|         1|\n",
            "|[5.67587520030241...|         1|\n",
            "|[5.55511189816831...|         1|\n",
            "|[6.03816510670469...|         1|\n",
            "+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "evaluator = ClusteringEvaluator(featuresCol=\"scaled_features\", predictionCol=\"prediction\")\n",
        "silhouette_score = evaluator.evaluate(predictions)\n",
        "print(f\"Silhouette Score = {silhouette_score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKKIG7cgCedn",
        "outputId": "57dc54b7-9c0c-4a71-ec0e-3c030dbf5d78"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score = 0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_scores = []\n",
        "for k in range(2, 10):\n",
        "    kmeans.setK(k)\n",
        "    model = kmeans.fit(scaled_df)\n",
        "    predictions = model.transform(scaled_df)\n",
        "    score = evaluator.evaluate(predictions)\n",
        "    silhouette_scores.append(score)\n",
        "    print(f\"Silhouette Score for k={k} = {score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_21VGMDCoWu",
        "outputId": "0d659866-d216-483c-e4f0-10b76b26cc8e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score for k=2 = 0.77\n",
            "Silhouette Score for k=3 = 0.64\n",
            "Silhouette Score for k=4 = 0.58\n",
            "Silhouette Score for k=5 = 0.52\n",
            "Silhouette Score for k=6 = 0.52\n",
            "Silhouette Score for k=7 = 0.51\n",
            "Silhouette Score for k=8 = 0.53\n",
            "Silhouette Score for k=9 = 0.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3)Build a Recommendation Engine with Spark with a dataset .\n"
      ],
      "metadata": {
        "id": "P5T3iaF9CzLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/ml-1m.zip -d /content/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aAWwvUmN3WR",
        "outputId": "384c023e-169b-4099-d24f-7ee1d04576bb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/ml-1m.zip\n",
            "   creating: /content/ml-1m/\n",
            "  inflating: /content/ml-1m/movies.dat  \n",
            "  inflating: /content/ml-1m/ratings.dat  \n",
            "  inflating: /content/ml-1m/README   \n",
            "  inflating: /content/ml-1m/users.dat  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_raw = spark.read.text(\"/content/ml-1m/ratings.dat\")\n"
      ],
      "metadata": {
        "id": "YS6yRSywN4te"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Unzip the dataset (only do this once)\n",
        "!unzip /content/ml-1m.zip -d /content/\n",
        "\n",
        "# Step 2: Read the unzipped ratings.dat file\n",
        "ratings_raw = spark.read.text(\"/content/ml-1m/ratings.dat\")\n",
        "\n",
        "# Step 3: Parse data into structured format\n",
        "ratings = ratings_raw.select(\n",
        "    split(ratings_raw.value, \"::\").getItem(0).cast(\"int\").alias(\"userId\"),\n",
        "    split(ratings_raw.value, \"::\").getItem(1).cast(\"int\").alias(\"movieId\"),\n",
        "    split(ratings_raw.value, \"::\").getItem(2).cast(\"float\").alias(\"rating\")\n",
        ").filter(col(\"userId\").isNotNull() & col(\"movieId\").isNotNull())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsetvXv8N7fe",
        "outputId": "b563ffa5-073f-4aa5-e892-4235355830da"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/ml-1m.zip\n",
            "replace /content/ml-1m/movies.dat? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: /content/ml-1m/movies.dat  \n",
            "replace /content/ml-1m/ratings.dat? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: /content/ml-1m/ratings.dat  \n",
            "replace /content/ml-1m/README? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: /content/ml-1m/README   \n",
            "replace /content/ml-1m/users.dat? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: /content/ml-1m/users.dat  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ratings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th6LYUXLOH0_",
        "outputId": "0c0d6212-96e5-47d9-c450-0afbf50647bd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[userId: int, movieId: int, rating: float]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_raw.show(5, truncate=False)\n",
        "print(f\"Total records: {ratings_raw.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75WHzpcwOupU",
        "outputId": "623c7c2c-cabb-4b67-c48a-e1c1872e6583"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+\n",
            "|value                |\n",
            "+---------------------+\n",
            "|1::1193::5::978300760|\n",
            "|1::661::3::978302109 |\n",
            "|1::914::3::978301968 |\n",
            "|1::3408::4::978300275|\n",
            "|1::2355::5::978824291|\n",
            "+---------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Total records: 1000209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, col\n",
        "\n",
        "# Split the 'value' column using \"::\" and cast the parts into appropriate types\n",
        "ratings = ratings_raw.select(\n",
        "    split(col(\"value\"), \"::\").getItem(0).cast(\"int\").alias(\"userId\"),\n",
        "    split(col(\"value\"), \"::\").getItem(1).cast(\"int\").alias(\"movieId\"),\n",
        "    split(col(\"value\"), \"::\").getItem(2).cast(\"float\").alias(\"rating\")\n",
        ").na.drop()  # drop rows with nulls (just in case)\n",
        "\n",
        "# Optional: check the parsed result\n",
        "ratings.show(5)\n",
        "ratings.printSchema()\n",
        "print(f\"Parsed records: {ratings.count()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LouPO-k-O5FO",
        "outputId": "b1282fb2-099e-4304-82b4-d4061e26330d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+------+\n",
            "|userId|movieId|rating|\n",
            "+------+-------+------+\n",
            "|     1|   1193|   5.0|\n",
            "|     1|    661|   3.0|\n",
            "|     1|    914|   3.0|\n",
            "|     1|   3408|   4.0|\n",
            "|     1|   2355|   5.0|\n",
            "+------+-------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- userId: integer (nullable = true)\n",
            " |-- movieId: integer (nullable = true)\n",
            " |-- rating: float (nullable = true)\n",
            "\n",
            "Parsed records: 1000209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "train, test = ratings.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "als = ALS(\n",
        "    userCol=\"userId\",\n",
        "    itemCol=\"movieId\",\n",
        "    ratingCol=\"rating\",\n",
        "    coldStartStrategy=\"drop\",\n",
        "    nonnegative=True\n",
        ")\n",
        "\n",
        "model = als.fit(train)\n",
        "\n",
        "predictions = model.transform(test)\n",
        "\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName=\"rmse\",\n",
        "    labelCol=\"rating\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"RMSE: {rmse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1HUedtZO8eN",
        "outputId": "b4801ba1-4b76-4344-b775-220c1896b206"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.8711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load movie titles\n",
        "movies_raw = spark.read.text(\"ml-1m/movies.dat\")\n",
        "movies = movies_raw.select(\n",
        "    split(col(\"value\"), \"::\").getItem(0).cast(\"int\").alias(\"movieId\"),\n",
        "    split(col(\"value\"), \"::\").getItem(1).alias(\"title\")\n",
        ")\n",
        "\n",
        "# Explode recommendations into rows\n",
        "from pyspark.sql.functions import explode\n",
        "exploded_recs = user_recs.select(\"userId\", explode(\"recommendations\").alias(\"rec\"))\n",
        "exploded_recs = exploded_recs.select(\"userId\", col(\"rec.movieId\"), col(\"rec.rating\"))\n",
        "\n",
        "# Join with movie titles\n",
        "final_recs = exploded_recs.join(movies, on=\"movieId\", how=\"left\")\n",
        "final_recs.orderBy(\"userId\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9PgPaLCSlyA",
        "outputId": "7d3ea358-bee9-467f-c672-b161e6b61e73"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+---------+------------------------------------------+\n",
            "|movieId|userId|rating   |title                                     |\n",
            "+-------+------+---------+------------------------------------------+\n",
            "|3603   |148   |4.708707 |Gay Deceivers, The (1969)                 |\n",
            "|3314   |148   |4.495983 |Big Trees, The (1952)                     |\n",
            "|1198   |148   |4.4924755|Raiders of the Lost Ark (1981)            |\n",
            "|439    |148   |4.4616227|Dangerous Game (1993)                     |\n",
            "|598    |148   |4.450812 |Window to Paris (1994)                    |\n",
            "|3338   |463   |4.1977086|For All Mankind (1989)                    |\n",
            "|2309   |463   |4.167856 |Inheritors, The (Die Siebtelbauern) (1998)|\n",
            "|2905   |463   |4.1588063|Sanjuro (1962)                            |\n",
            "|858    |463   |4.141514 |Godfather, The (1972)                     |\n",
            "|2760   |463   |4.1289897|Gambler, The (A J�t�kos) (1997)           |\n",
            "|2309   |471   |4.7961698|Inheritors, The (Die Siebtelbauern) (1998)|\n",
            "|3245   |471   |4.671932 |I Am Cuba (Soy Cuba/Ya Kuba) (1964)       |\n",
            "|2905   |471   |4.6269174|Sanjuro (1962)                            |\n",
            "|3338   |471   |4.5996   |For All Mankind (1989)                    |\n",
            "|670    |471   |4.575251 |World of Apu, The (Apur Sansar) (1959)    |\n",
            "|108    |496   |5.3121986|Catwalk (1995)                            |\n",
            "|3625   |496   |5.1858387|Better Living Through Circuitry (1999)    |\n",
            "|1780   |496   |5.139661 |Ayn Rand: A Sense of Life (1997)          |\n",
            "|598    |496   |5.041111 |Window to Paris (1994)                    |\n",
            "|2127   |496   |5.0168076|First Love, Last Rites (1997)             |\n",
            "+-------+------+---------+------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#movie recommendation for the 5 unique users with the movie names:\n",
        "user_subset = ratings.select(\"userId\").distinct().limit(5)\n",
        "user_recs = model.recommendForUserSubset(user_subset, 5)\n",
        "movies_raw = spark.read.text(\"ml-1m/movies.dat\")\n",
        "\n",
        "movies = movies_raw.select(\n",
        "    split(col(\"value\"), \"::\").getItem(0).cast(\"int\").alias(\"movieId\"),\n",
        "    split(col(\"value\"), \"::\").getItem(1).alias(\"title\")\n",
        ")\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "# Explode the nested recommendations array\n",
        "exploded_recs = user_recs.select(\n",
        "    col(\"userId\"),\n",
        "    explode(col(\"recommendations\")).alias(\"rec\")\n",
        ").select(\n",
        "    col(\"userId\"),\n",
        "    col(\"rec.movieId\"),\n",
        "    col(\"rec.rating\")\n",
        ")\n",
        "\n",
        "# Join with movie titles\n",
        "final_recs = exploded_recs.join(movies, on=\"movieId\", how=\"left\")\n",
        "\n",
        "# Show the recommendations\n",
        "final_recs.orderBy(\"userId\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqLMe_NfTFBG",
        "outputId": "8ec59ebb-5a8a-4eb7-800f-03f691e1fcb7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+---------+------------------------------------------+\n",
            "|movieId|userId|rating   |title                                     |\n",
            "+-------+------+---------+------------------------------------------+\n",
            "|3603   |148   |4.708707 |Gay Deceivers, The (1969)                 |\n",
            "|3314   |148   |4.495983 |Big Trees, The (1952)                     |\n",
            "|1198   |148   |4.4924755|Raiders of the Lost Ark (1981)            |\n",
            "|439    |148   |4.4616227|Dangerous Game (1993)                     |\n",
            "|598    |148   |4.450812 |Window to Paris (1994)                    |\n",
            "|3338   |463   |4.1977086|For All Mankind (1989)                    |\n",
            "|2309   |463   |4.167856 |Inheritors, The (Die Siebtelbauern) (1998)|\n",
            "|2905   |463   |4.1588063|Sanjuro (1962)                            |\n",
            "|858    |463   |4.141514 |Godfather, The (1972)                     |\n",
            "|2760   |463   |4.1289897|Gambler, The (A J�t�kos) (1997)           |\n",
            "|2309   |471   |4.7961698|Inheritors, The (Die Siebtelbauern) (1998)|\n",
            "|3245   |471   |4.671932 |I Am Cuba (Soy Cuba/Ya Kuba) (1964)       |\n",
            "|2905   |471   |4.6269174|Sanjuro (1962)                            |\n",
            "|3338   |471   |4.5996   |For All Mankind (1989)                    |\n",
            "|670    |471   |4.575251 |World of Apu, The (Apur Sansar) (1959)    |\n",
            "|108    |496   |5.3121986|Catwalk (1995)                            |\n",
            "|3625   |496   |5.1858387|Better Living Through Circuitry (1999)    |\n",
            "|1780   |496   |5.139661 |Ayn Rand: A Sense of Life (1997)          |\n",
            "|598    |496   |5.041111 |Window to Paris (1994)                    |\n",
            "|2127   |496   |5.0168076|First Love, Last Rites (1997)             |\n",
            "+-------+------+---------+------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}